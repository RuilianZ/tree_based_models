---
title: "Data Science II Homework 4"
author: "Roxy Zhang"
date: "4/1/2022"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(caret)
library(GGally)
library(gridExtra)
library(pROC)
library(ISLR)
library(rpart) # CART algorithm - Classification And Regression Trees
library(rpart.plot) # visualization
library(party) # CIT - Conditional Inference Tree
library(partykit) # plotting
library(randomForest)
library(ranger)
library(gbm)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.align = "center", cache = TRUE) 
                      #fig.width = 6, fig.asp = 0.6, out.width = "90%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Question 1

```{r}
set.seed(0409)

# data import and cleaning
df = read_csv("College.csv") %>% 
  janitor::clean_names() %>% 
  select(-college) %>% 
  select(outstate, everything()) %>% 
  na.omit()

# data partition
indexTrain = createDataPartition(y = df$outstate,
                                 p = 0.8,
                                 list = FALSE)


train_df = df[indexTrain, ]
test_df = df[-indexTrain, ]
```

```{r}
# data dimension and summary
dim(train_df)

summary(train_df)
```

There are 453 rows and 17 columns in training data, all the variables are numeric.


**(a) Build a regression tree on the training data to predict the response. Create a plot of the tree.**

```{r}
set.seed(0409)

reg_tree = rpart(formula = outstate ~ . ,
                 data = train_df,
                 control = rpart.control(cp = 0)) 

# cp table
reg_tree_cptable = reg_tree$cptable

# cross-validation plot
plotcp(reg_tree)

# minimum cross_validation error
min_err = which.min(reg_tree_cptable[,4])

# pruning
reg_tree_prune = prune(reg_tree, 
                       cp = reg_tree_cptable[min_err,1])

#summary(reg_tree_prune) 
```

```{r}
# make prediction
reg_tree_pred = predict(reg_tree_prune, newdata = test_df)

head(reg_tree_pred)

RMSE(reg_tree_pred, test_df$outstate)
```


**(b) Perform random forest on the training data. Report the variable importance and the test error.**

```{r}
set.seed(0409)

# using randomForest
rf = randomForest(outstate ~ .,
                  data = train_df,
                  mtry = 6)

rf_pred = predict(rf, newdata = test_df)

RMSE(rf_pred, test_df$outstate)
```

```{r}
set.seed(0409)

# fast implementation using ranger
rf2 = ranger(outstate ~ .,
             data = train_df,
             mtry = 6)

rf2_pred = predict(rf2, data = test_df)$predictions

# test error
RMSE(rf2_pred, test_df$outstate)
```

* The function `randomForest()` implements Breiman's random forest algorithm. The test error is `r RMSE(rf_pred, test_df$outstate)`.  
* `ranger()` is a fast implementation of the algorithm above, particularly suit for high dimentional data. The test error is `r RMSE(rf2_pred, test_df$outstate)`.


```{r}
set.seed(0409)

# train random forest model using caret
ctrl = trainControl(method = "cv")

rf_grid = expand.grid(mtry = seq(1, 16, 3),
                      splitrule = "variance",
                      min.node.size = 1:12)

rf_grid_fit = train(outstate ~ .,
               data = train_df,
               method = "ranger",
               tuneGrid = rf_grid,
               trControl = ctrl)

rf_grid_fit$bestTune

ggplot(rf_grid_fit, highlight = TRUE)
```

```{r}
set.seed(0409)

# extract variable importance using permutation
rf_per = ranger(outstate ~ . , 
                data = train_df,
                mtry = rf_grid_fit$bestTune[[1]],
                splitrule = "variance",
                min.node.size = rf_grid_fit$bestTune[[3]],
                importance = "permutation",
                scale.permutation.importance = TRUE)

# variable importance
barplot(sort(ranger::importance(rf_per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7, 
        col = colorRampPalette(colors = c("cyan", "blue"))(19))
```

```{r}
# make prediction
rf_grid_pred = predict(rf_grid_fit, newdata = test_df)

# test error
RMSE(rf_grid_pred, test_df$outstate)
```


**(c) Perform boosting on the training data. Report the variable importance and the test error.**

```{r}
set.seed(0409)

# fit a gradient boosting model with Gaussian loss function
boost = gbm(outstate ~ .,
            data = train_df,
            distribution = "gaussian",
            n.trees = 2000,
            interaction.depth = 3,
            shrinkage = 0.005,
            cv.folds = 10,
            n.cores = 2)

# plot loss function as a result of number of trees added to the ensemble
gbm.perf(boost, method = "cv")

best.iter = 1669

# check performance using the out-of-bag (OOB) error
# the OOB error typically underestimates the optimal number of iterations
gbm.perf(boost, method = "OOB")
```

* The green curve represents the cross-validation error, and the black curve represents the training error.   
* The best cross-validation iteration was `r best.iter`, as is shown by the vertical dash line.

```{r}
# plot relative influence of each variable
par(mfrow = c(1, 2))
summary(boost, n.trees = 1) # using first tree
summary(boost, n.trees = best.iter) # using estimated best number of trees
```

* The left plot shows the variable influence of the first tree, the right plot shows the variable influence of the estimated best number of trees.  
* `expend`, `apps`, `ph_d`, `top10perc`, and `books` are important variables, which are consistent with the variable importance in caret.

```{r}
# predict on the new data using the "best" number of trees
# by default, predictions will be on the link scale
boost_pred = predict(boost,
                     newdata = test_df,
                     n.trees = best.iter,
                     type = "link")

# test error
RMSE(boost_pred, test_df$outstate)
```

* The test error is `r RMSE(boost_pred, test_df$outstate)`, which is smaller than the test error `r RMSE(rf_grid_pred, test_df$outstate)` from the tuned model from caret.



## Question 2

```{r}
# data import and cleaning
data(OJ)

OJ_df = OJ %>% 
  janitor::clean_names() %>% 
  relocate("purchase", .after = "store") %>% 
  mutate(purchase = as.factor(purchase)) %>% 
  na.omit()

dim(OJ_df)

summary(OJ_df)
```

```{r}
set.seed(0409)

# data partition
OJ_indexTrain = createDataPartition(y = OJ_df$purchase,
                                 p = 0.653,
                                 list = FALSE)

OJ_train_df = OJ_df[OJ_indexTrain, ]
OJ_test_df = OJ_df[-OJ_indexTrain, ]
```


**(a) Build a classification tree using the training data, with Purchase as the response and the other variables as predictors. Use cross-validation to determine the tree size and create a plot of the final tree. Which tree size corresponds to the lowest cross-validation error? Is this the same as the tree size obtained using the 1 SE rule?**

**Min MSE rule:**

```{r}
set.seed(0409)

# fit classification model using rpart
class_tree = rpart(purchase ~ . , 
                   data = OJ_train_df,
                   control = rpart.control(cp = 0))

# tract cp table
OJ_cp_table = printcp(class_tree)

# extract min MSE
OJ_min_MSE = which.min(OJ_cp_table[ , 4])

# plot cross-validation error agiainst cp
plotcp(class_tree)

# obtain final tree using min MSE
cp_MSE = OJ_cp_table[OJ_min_MSE, 1]

class_tree_prune = prune(class_tree, cp = cp_MSE)

# plot final tree
rpart.plot(class_tree_prune)
```

**1 SE rule:**

```{r}
set.seed(0409)

# obtain final tree using 1SE
cp_1SE = OJ_cp_table[OJ_cp_table[ , 4] < OJ_cp_table[OJ_min_MSE, 4] + OJ_cp_table[OJ_min_MSE, 5], 1][1]

class_tree_prune_1SE = prune(class_tree, 
                             cp = cp_1SE)

# plot fianl tree
rpart.plot(class_tree_prune_1SE)
```


**(b) Perform boosting on the training data and report the variable importance. What is the test error rate?**

```{r}

```

